from flask import Flask, jsonify, request 
from flask_restful import Resource, Api 
from flask import render_template, url_for, request
from urllib.parse import unquote

import pandas as pd
import numpy as np
import random

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from joblib import dump, load


# creating the flask app 
app = Flask(__name__) 
# creating an API object 
api = Api(app) 
class Hello(Resource): 
  
    # corresponds to the GET request. 
    # this function is called whenever there 
    # is a GET request for this resource 
    def get(self):
        urls_data = pd.read_csv("urldata.csv")
        y = urls_data["label"]
        print (y)
        url_list = urls_data["url"]

        def Tokens(f):
            tkns_BySlash = str(f.encode('utf-8')).split('/')	# make tokens after splitting by slash
            total_Tokens = []
            for i in tkns_BySlash:
                tokens = str(i).split('-')	# make tokens after splitting by dash
                tkns_ByDot = []
                for j in range(0,len(tokens)):
                    temp_Tokens = str(tokens[j]).split('.')	# make tokens after splitting by dot
                    tkns_ByDot = tkns_ByDot + temp_Tokens
                total_Tokens = total_Tokens + tokens + tkns_ByDot
            total_Tokens = list(set(total_Tokens))	#remove redundant tokens
            if 'com' in total_Tokens:
                total_Tokens.remove('com')	#removing .com since it occurs a lot of times and it should not be included in our features
            return total_Tokens
        
        
        
        vectorizer = TfidfVectorizer(tokenizer=Tokens)

        x = vectorizer.fit_transform(url_list)
        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
        logit = LogisticRegression()	
        logit.fit(x_train, y_train)

        #print("Accuracy ",logit.score(x_test, y_test))
        #dump(logit, 'model.joblib')
        
        model = load('model.joblib')
        #url = "rediff.com/a.exe"
        #decurl = str(url,"utf-8")
        url =  request.args.get('url')
        print(url)
        decurl = unquote(url)
        print (decurl)

        data = [decurl]
        vect = vectorizer.transform(data).toarray()
        my_prediction = model.predict(vect)
        print (my_prediction)
        if my_prediction == "bad":
            return jsonify({'status': 0})
        elif my_prediction == "good":
            return jsonify({'status': 1})
                    #return render_template("result.html", prediction=my_prediction, phish=url)


        #return jsonify({'status': my_prediction})
            #data = [url]
            # vect = vectorizer.transform(data).toarray()
            # my_prediction = model.predict(vect)
            #
        #return jsonify({'status':url**2 }) 

    # Corresponds to POST request 
    # def post(self): 
          
    #     data = request.get_json()     # status code 
    #     return jsonify({'data': data}), 201
  
  
# another resource to calculate the square of a number 
  
  
# adding the defined resources along with their corresponding urls 
api.add_resource(Hello, '/predicturl') 
  
  
# driver function 
if __name__ == '__main__': 
  
    app.run(debug = True) 